{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Create a word-level tokenizer with different splitting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Word-Level Tokenization:\n",
    "1. Preserves word-level semantic meaning.\n",
    "2. Shorter sequences compared to character-level tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of Word-Level Tokenization:\n",
    "1. Larger vocabulary size - need to represent many unique words.\n",
    "2. Different splitting rules can significantly affect tokenization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the text from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
     ]
    }
   ],
   "source": [
    "# Load the text\n",
    "with open(\n",
    "    \"/Users/sadiahzahoor/Desktop/AI Research/LLMs /LLM's from Scratch/the-verdict.txt\",\n",
    "    \"r\",\n",
    ") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Total number of characters in the text: \", len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define some splitting rules, various regex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Patterns for Splitting Rules\n",
    "1. None (Whitespace)\n",
    "- This is not a regex pattern but a placeholder.\n",
    "- Likely used to indicate that whitespace should be ignored or not tokenized.\n",
    "\n",
    "---\n",
    "\n",
    "2. r'\\b[a-zA-Z]+\\b' (Word Only)\n",
    "- Matches words containing only *alphabetic characters* (a-z, A-Z).\n",
    "- \\b → Ensures the word is *bounded* (i.e., it starts and ends at a word boundary).\n",
    "- [a-zA-Z]+ → Matches *one or more* (+) alphabetic characters.\n",
    "- *Example Matches*:\n",
    "  - ✅ \"hello\"\n",
    "  - ✅ \"Test\"\n",
    "  - ✅ \"World\"\n",
    "- *Does Not Match*:\n",
    "  - ❌ \"123\" (no letters)\n",
    "  - ❌ \"hello123\" (contains numbers)\n",
    "  - ❌ \"email@domain.com\" (contains special characters)\n",
    "\n",
    "---\n",
    "\n",
    "3. r'\\b[a-zA-Z0-9]+\\b' (Word or Number)\n",
    "- Similar to the previous pattern but allows *numbers*.\n",
    "- [a-zA-Z0-9]+ → Matches *one or more* letters (a-z, A-Z) or digits (0-9).\n",
    "- *Example Matches*:\n",
    "  - ✅ \"hello\"\n",
    "  - ✅ \"test123\"\n",
    "  - ✅ \"2024\"\n",
    "- *Does Not Match*:\n",
    "  - ❌ \"hello-123\" (contains a hyphen)\n",
    "  - ❌ \"email@domain.com\" (contains special characters)\n",
    "\n",
    "---\n",
    "\n",
    "4. r'\\b[a-zA-Z0-9]+(?:-[a-zA-Z]+)*+\\b' (Words with Hyphens)\n",
    "- Allows *hyphenated words*.\n",
    "- \\b → Ensures word boundary.\n",
    "- [a-zA-Z0-9]+ → Matches *a word with letters or numbers*.\n",
    "- (?:-[a-zA-Z]+)*+ → Allows *hyphenated parts* (-word) *zero or more times* (*+).\n",
    "- *Example Matches*:\n",
    "  - ✅ \"high-quality\"\n",
    "  - ✅ \"multi-purpose\"\n",
    "  - ✅ \"user-friendly\"\n",
    "- *Does Not Match*:\n",
    "  - ❌ \"hello-\" (trailing hyphen)\n",
    "  - ❌ \"-hello\" (leading hyphen)\n",
    "  - ❌ \"123-456\" (numbers after hyphen not allowed)\n",
    "\n",
    "---\n",
    "\n",
    "5. r'\\b[a-zA-Z0-9]+\\b|[.,!?;;:]' (Word or Punctuation as Separate Tokens)\n",
    "- Matches *either*:\n",
    "  - Words (\\b[a-zA-Z0-9]+\\b)\n",
    "  - | -> OR\n",
    "  - OR punctuation characters ([.,!?;;:])\n",
    "- *Example Matches*:\n",
    "  - ✅ \"hello\"\n",
    "  - ✅ \"world\"\n",
    "  - ✅ \"123\"\n",
    "  - ✅ \"!\", \".\", \";\"\n",
    "- *Does Not Match*:\n",
    "  - ❌ \"email@domain.com\" (contains @)\n",
    "  - ❌ \"hello-world\" (hyphen not included in this pattern)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'world!', 'This', 'is', 'a', 'test.']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to tokenize text using a given regex pattern\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize_with_pattern(text, pattern=None):\n",
    "    if pattern is None:\n",
    "        return text.split()\n",
    "    else:\n",
    "        return re.findall(pattern, text)\n",
    "    \n",
    "# Example:\n",
    "text = \"Hello, world! This is a test.\"\n",
    "\n",
    "# Tokenize with whitespace pattern (default)\n",
    "tokens = tokenize_with_pattern(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "# Tokenise with a particular pattern\n",
    "\n",
    "# Define patterns for different splitting rules\n",
    "\n",
    "patterns = {\n",
    "    \"whitespace\": None,  # Use basic whitespace splitting\n",
    "    \"word_only\": r\"\\b[a-zA-Z]+\\b\",  # Word only (letters)\n",
    "    \"alphanumeric\": r\"\\b[a-zA-Z0-9]+\\b\",  # Word or number\n",
    "    \"hyphenated\": r\"\\b[a-zA-Z0-9]+(?:-[a-zA-Z0-9]+)*\\b\",  # Words with hyphens\n",
    "    \"word_and_punct\": r\"[a-zA-Z0-9]+|[.,!?;:]\",  # Word or punctuation as separate tokens\n",
    "}\n",
    "\n",
    "\n",
    "# Example:\n",
    "text = \"Hello, world! This is a test.\"\n",
    "pattern = patterns.get(\"word_only\")\n",
    "tokens = tokenize_with_pattern(text, pattern)\n",
    "print(tokens)\n",
    "\n",
    "# We see no punctuation is included in the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define a general tokenizer class that handles any regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'This', 'is', 'a', 'test']\n",
      "['Hello', 'This', 'a', 'is', 'test', 'world']\n",
      "[0, 5, 1, 3, 2, 4]\n",
      "{'Hello': 0, 'This': 1, 'a': 2, 'is': 3, 'test': 4, 'world': 5}\n",
      "Encoding of Example 1:  [0, 5, 1, 3, 2, 4]\n",
      "Decoding of Example 1:  Hello world This is a test\n",
      "Encoding of Example 2:  [1, 3, 6, 7, 8, 9, 10, 11]\n",
      "Decoding of Example 2:  This is the best way to tokenize text\n"
     ]
    }
   ],
   "source": [
    "# Define tokenization patterns outside the class, Keep adding more if needed.\n",
    "TOKENIZATION_PATTERNS = {\n",
    "    \"whitespace\": None,  # Use basic whitespace splitting\n",
    "    \"word_only\": r\"\\b[a-zA-Z]+\\b\",  # Word only (letters)\n",
    "    \"alphanumeric\": r\"\\b[a-zA-Z0-9]+\\b\",  # Word or number\n",
    "    \"hyphenated\": r\"\\b[a-zA-Z0-9]+(?:-[a-zA-Z0-9]+)*\\b\",  # Words with hyphens\n",
    "    \"word_and_punct\": r\"\\b[a-zA-Z0-9]+\\b|[.,!?;:]\",  # Word or punctuation as separate tokens\n",
    "}\n",
    "\n",
    "class PatternTokenizer:\n",
    "    def __init__(self, text, pattern_type=None):\n",
    "        self.text = text\n",
    "        self.pattern_type = pattern_type\n",
    "        self.pattern = TOKENIZATION_PATTERNS.get(pattern_type, None)\n",
    "\n",
    "        # Method 1: Split the text into tokens based on the pattern\n",
    "        self.tokens = self._tokenize(\n",
    "            text\n",
    "        ) \n",
    "        # Sort the tokens and remove duplicates\n",
    "        self.unique_tokens = sorted(\n",
    "            list(set(self.tokens))\n",
    "        )  \n",
    "\n",
    "        # Method 2: Create mapping dictionaries\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.unique_tokens)}\n",
    "\n",
    "    def _tokenize(self,text):\n",
    "        if self.pattern is None:\n",
    "            return text.split()\n",
    "        else:\n",
    "            return re.findall(self.pattern, text)\n",
    "\n",
    "    def encode(self,text):\n",
    "        tokens = self._tokenize(text)\n",
    "        token_ids = []\n",
    "\n",
    "        # Handle tokens not seen during initialization and add them to the vocabulary\n",
    "        for token in tokens:\n",
    "            if token not in self.token_to_id:\n",
    "                self.token_to_id[token] = len(self.token_to_id)\n",
    "            token_ids.append(self.token_to_id[token])\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self,token_ids):\n",
    "        tokens = []\n",
    "        # Create a reverse mapping dictionary based on updated token_to_id\n",
    "        self.id_to_token = {id: token for token, id in self.token_to_id.items()}\n",
    "\n",
    "        # Decode the token ids back to tokens\n",
    "        for token_id in token_ids:\n",
    "            tokens.append(self.id_to_token[token_id]) \n",
    "\n",
    "        # Join tokens with spaces\n",
    "        return \" \".join(tokens)  \n",
    "\n",
    "\n",
    "# Pattern 1: Word Only\n",
    "# Example 1:\n",
    "text = \"Hello, world! This is a test.\"\n",
    "tokenizer = PatternTokenizer(text, \"word_only\")\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.unique_tokens)\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded)\n",
    "print(tokenizer.token_to_id)\n",
    "\n",
    "# Decode the text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Encoding of Example 1: \", encoded)\n",
    "print(\"Decoding of Example 1: \", decoded)\n",
    "\n",
    "# Example 2:\n",
    "text_2 = \"This is the best way to tokenize text.\"\n",
    "encoded_2 = tokenizer.encode(text_2)\n",
    "print(\"Encoding of Example 2: \", encoded_2)\n",
    "\n",
    "decoded_2 = tokenizer.decode(encoded_2)\n",
    "print(\"Decoding of Example 2: \", decoded_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
      "['!', ',', '.', 'Hello', 'This', 'a', 'is', 'test', 'world']\n",
      "Encoding of Example 1:  [3, 1, 8, 0, 4, 6, 5, 7, 2]\n",
      "Token to ID of Example 1:  {'!': 0, ',': 1, '.': 2, 'Hello': 3, 'This': 4, 'a': 5, 'is': 6, 'test': 7, 'world': 8}\n",
      "Decoding of Example 1:  Hello , world ! This is a test .\n",
      "Encoding of Example 2:  [4, 6, 9, 10, 11, 12, 13, 14, 2]\n",
      "Token to ID of Example 2:  {'!': 0, ',': 1, '.': 2, 'Hello': 3, 'This': 4, 'a': 5, 'is': 6, 'test': 7, 'world': 8, 'the': 9, 'best': 10, 'way': 11, 'to': 12, 'tokenize': 13, 'text': 14}\n",
      "Decoding of Example 2:  This is the best way to tokenize text .\n"
     ]
    }
   ],
   "source": [
    "# Pattern 2: Word and Punctuation\n",
    "\n",
    "# Get new tokenizer for word and punctuation pattern\n",
    "text = \"Hello, world! This is a test.\"\n",
    "tokenizer_2 = PatternTokenizer(text, \"word_and_punct\")\n",
    "print(tokenizer_2.tokens)\n",
    "print(tokenizer_2.unique_tokens)\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer_2.encode(text)\n",
    "print(\"Encoding of Example 1: \", encoded)\n",
    "print(\"Token to ID of Example 1: \", tokenizer_2.token_to_id)\n",
    "\n",
    "# Decode the text\n",
    "decoded = tokenizer_2.decode(encoded)\n",
    "print(\"Decoding of Example 1: \", decoded)\n",
    "\n",
    "# Example 2:\n",
    "\n",
    "# Encode the text\n",
    "encoded_2 = tokenizer_2.encode(text_2)\n",
    "print(\"Encoding of Example 2: \", encoded_2)\n",
    "print(\"Token to ID of Example 2: \", tokenizer_2.token_to_id)\n",
    "\n",
    "# Decode the text\n",
    "decoded_2 = tokenizer_2.decode(encoded_2)\n",
    "print(\"Decoding of Example 2: \", decoded_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'hyphenated-word']\n",
      "['This', 'a', 'hyphenated-word', 'is']\n",
      "Encoding of Example 1:  [0, 3, 1, 2]\n",
      "Token to ID of Example 1:  {'This': 0, 'a': 1, 'hyphenated-word': 2, 'is': 3}\n",
      "Decoding of Example 1:  This is a hyphenated-word\n",
      "Encoding of Example 2:  [0, 4, 5, 6, 7, 8, 3, 9, 10, 11, 8, 3, 12]\n",
      "Token to ID of Example 2:  {'This': 0, 'a': 1, 'hyphenated-word': 2, 'is': 3, 'has': 4, 'many': 5, 'hyphenated-words': 6, 'Like': 7, 'one': 8, 'bigger-than-the-other': 9, 'and': 10, 'another': 11, 'smaller-than-the-other': 12}\n",
      "Decoding of Example 2:  This has many hyphenated-words Like one is bigger-than-the-other and another one is smaller-than-the-other\n"
     ]
    }
   ],
   "source": [
    "# Pattern 3: Hyphenated\n",
    "\n",
    "# Get new tokenizer for hyphenated pattern\n",
    "text = \"This is a hyphenated-word.\"\n",
    "tokenizer_3 = PatternTokenizer(text, \"hyphenated\")\n",
    "print(tokenizer_3.tokens)\n",
    "print(tokenizer_3.unique_tokens)\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer_3.encode(text)\n",
    "print(\"Encoding of Example 1: \", encoded)\n",
    "print(\"Token to ID of Example 1: \", tokenizer_3.token_to_id)\n",
    "\n",
    "# Decode the text\n",
    "decoded = tokenizer_3.decode(encoded)\n",
    "print(\"Decoding of Example 1: \", decoded)\n",
    "\n",
    "# Example 2:\n",
    "\n",
    "text_2 = \"This has many hyphenated-words. Like one is bigger-than-the-other, and another one is smaller-than-the-other.\"\n",
    "encoded_2 = tokenizer_3.encode(text_2)\n",
    "print(\"Encoding of Example 2: \", encoded_2)\n",
    "print(\"Token to ID of Example 2: \", tokenizer_3.token_to_id)\n",
    "\n",
    "decoded_2 = tokenizer_3.decode(encoded_2)\n",
    "print(\"Decoding of Example 2: \", decoded_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'subword', 'like', 'tokenization', 'It', 'has', '1', '23', '456', '7890', 'and', '1234567890']\n",
      "['1', '1234567890', '23', '456', '7890', 'It', 'This', 'a', 'and', 'has', 'is', 'like', 'subword', 'tokenization']\n",
      "Encoding of Example 1:  [6, 10, 7, 12, 11, 13, 5, 9, 0, 2, 3, 4, 8, 1]\n",
      "Token to ID of Example 1:  {'1': 0, '1234567890': 1, '23': 2, '456': 3, '7890': 4, 'It': 5, 'This': 6, 'a': 7, 'and': 8, 'has': 9, 'is': 10, 'like': 11, 'subword': 12, 'tokenization': 13}\n",
      "Decoding of Example 1:  This is a subword like tokenization It has 1 23 456 7890 and 1234567890\n",
      "Encoding of Example 2:  [6, 10, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "Token to ID of Example 2:  {'1': 0, '1234567890': 1, '23': 2, '456': 3, '7890': 4, 'It': 5, 'This': 6, 'a': 7, 'and': 8, 'has': 9, 'is': 10, 'like': 11, 'subword': 12, 'tokenization': 13, 'the': 14, 'best': 15, 'way': 16, 'we': 17, 'can': 18, 'do': 19, 'this': 20, '234567890': 21}\n",
      "Decoding of Example 2:  This is the best way we can do this 234567890\n"
     ]
    }
   ],
   "source": [
    "# Pattern 4: Alphanumeric\n",
    "\n",
    "# Get new tokenizer for subword pattern\n",
    "text = \"This is a subword-like tokenization. It has 1, 23, 456, 7890 and 1234567890\"\n",
    "tokenizer_4 = PatternTokenizer(text, \"alphanumeric\")\n",
    "print(tokenizer_4.tokens)\n",
    "print(tokenizer_4.unique_tokens)\n",
    "\n",
    "# Example 1:\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer_4.encode(text)\n",
    "print(\"Encoding of Example 1: \", encoded)\n",
    "print(\"Token to ID of Example 1: \", tokenizer_4.token_to_id)\n",
    "\n",
    "# Decode the text\n",
    "decoded = tokenizer_4.decode(encoded)\n",
    "print(\"Decoding of Example 1: \", decoded)\n",
    "\n",
    "# Example 2:\n",
    "\n",
    "text_2 = \"This is the best way we can do this 234567890\"\n",
    "encoded_2 = tokenizer_4.encode(text_2)\n",
    "print(\"Encoding of Example 2: \", encoded_2)\n",
    "print(\"Token to ID of Example 2: \", tokenizer_4.token_to_id)\n",
    "\n",
    "decoded_2 = tokenizer_4.decode(encoded_2)\n",
    "print(\"Decoding of Example 2: \", decoded_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExploreLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
