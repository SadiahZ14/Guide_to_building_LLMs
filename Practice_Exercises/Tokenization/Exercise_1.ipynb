{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implementing a Character Level Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "1. No unknown tokens - every character in the input is tokenized.\n",
    "2. Smaller vocabulary size - only need to represent unique characters.\n",
    "\n",
    "### Disadvantages:\n",
    "1. Longer sequences - a sentence becomes many more tokens.\n",
    "2. May lose word-level semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "\n",
    "with open(\n",
    "    \"/Users/sadiahzahoor/Desktop/AI Research/LLMs /LLM's from Scratch/the-verdict.txt\",\n",
    "    \"r\",\n",
    ") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Total number of characters in the text: \", len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a character level Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique characters:  62\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text))) # Creates a list of unique characters in the text\n",
    "\n",
    "print(\"Total number of unique characters: \", len(unique_chars))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a character to token mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 0), (' ', 1), ('!', 2), ('\"', 3), (\"'\", 4), ('(', 5), (')', 6), (',', 7), ('-', 8), ('.', 9)]\n",
      "[(0, '\\n'), (1, ' '), (2, '!'), (3, '\"'), (4, \"'\"), (5, '('), (6, ')'), (7, ','), (8, '-'), (9, '.')]\n"
     ]
    }
   ],
   "source": [
    "char_to_token = {char: i for i, char in enumerate(unique_chars)}\n",
    "token_to_char = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Print first 10 items of the dictionary\n",
    "print(list(char_to_token.items())[:10]) # Creates a list of the first 10 items of the dictionary\n",
    "print(list(token_to_char.items())[:10]) # Creates a list of the first 10 items of the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a Character-Level-Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 40, 47, 47, 50, 7, 1, 58, 50, 53, 47, 39, 2]\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "class CharacterLevelTokenizerv1:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.unique_chars = sorted(list(set(text)))\n",
    "        self.char_to_token = {char: i for i, char in enumerate(self.unique_chars)}\n",
    "        self.token_to_char = {i: char for i, char in enumerate(self.unique_chars)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = [self.char_to_token[char] for char in text]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        words = ''.join([self.token_to_char[token_id] for token_id in token_ids])\n",
    "        return words\n",
    "\n",
    "\n",
    "# Test the CharacterLevelTokenizer\n",
    "tokenizer = CharacterLevelTokenizerv1(text)\n",
    "\n",
    "# Test encoding\n",
    "encoded = tokenizer.encode(\"Hello, world!\")\n",
    "print(encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks:\n",
    "\n",
    "The Character Level Tokenizer does not have any unknown tokens. This is because every character in the input is tokenized. As a result, the vocabulary size is equal to the number of unique characters in the text. But if we have a character that is not in the vocabulary, it will be tokenized as an unknown token. And throw an error. Say, in this case, we don't have numbers, so if we try to tokenize a number, it will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test the Unknown Token\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m encoded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, 10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(encoded)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mCharacterLevelTokenizerv1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     token_ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchar_to_token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token_ids\n",
      "\u001b[31mKeyError\u001b[39m: '1'"
     ]
    }
   ],
   "source": [
    "# Test the Unknown Token\n",
    "\n",
    "encoded = tokenizer.encode(\"Hello, 10\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement a Character Level Tokenizer with Unknown Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique characters: 63\n",
      "Last 5 characters of the unique characters: ['w', 'x', 'y', 'z', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "    \"/Users/sadiahzahoor/Desktop/AI Research/LLMs /LLM's from Scratch/the-verdict.txt\",\n",
    "    \"r\",\n",
    ") as file:\n",
    "    text = file.read()\n",
    "\n",
    "unique_chars = sorted(list(set(text)))\n",
    "extended_unique_chars = unique_chars + ['<|unk|>']\n",
    "\n",
    "print(f\"Total number of unique characters: {len(extended_unique_chars )}\")\n",
    "print(f\"Last 5 characters of the unique characters: {extended_unique_chars[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vocabulary\n",
    "char_to_token = {char: i for i, char in enumerate(extended_unique_chars)}\n",
    "token_to_char = {i: char for i, char in enumerate(extended_unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 40, 47, 47, 50, 7, 1, 62, 62]\n",
      "Hello, <|unk|><|unk|>\n"
     ]
    }
   ],
   "source": [
    "# Create a new tokenizer\n",
    "\n",
    "class CharacterLevelTokenizerv2:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.unique_chars = sorted(list(set(text))) + ['<|unk|>']\n",
    "        self.char_to_token = {char: i for i, char in enumerate(self.unique_chars)}\n",
    "        self.token_to_char = {i: char for i, char in enumerate(self.unique_chars)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # If the character is not in the vocabulary, it will be tokenized as <|unk|>\n",
    "        token_ids = [self.char_to_token[char] if char in self.char_to_token else self.char_to_token['<|unk|>'] for char in text]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        words = ''.join([self.token_to_char[token_id] for token_id in token_ids])\n",
    "        return words\n",
    "    \n",
    "# Test the CharacterLevelTokenizer with Unknown Tokens\n",
    "tokenizer = CharacterLevelTokenizerv2(text)\n",
    "\n",
    "# Test encoding\n",
    "encoded = tokenizer.encode(\"Hello, 10\")\n",
    "print(encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExploreLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
